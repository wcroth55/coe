<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta content="text/html; charset=ISO-8859-1" http-equiv="content-type">
  <title>Configuring a new COE RHEL/CentOS 6 linux system</title>
  <style type="text/css">
     body,td { font-family: arial;  font-size: 80%; }
     tt,pre { font-size: 120%; }
     
     LI { margin-top: .4em; }
  </style>
</head>
<body>

<h2>Configuring a new COE RHEL/CentOS 6 linux system</h2>
Charles Roth<br/>
Last update: 2014-12-12

<p/><b>I. Introduction</b><br>
This document 
describes specific steps taken to set up a new RHEL (Red Hat Enterprise Linux, 
or CentOS) version 6 server(s) from ServerBeach.&nbsp;
Basically, this is just a record of what we do to change the base 
configuration of the server as provided by ServerBeach.

<p/>
This is a work-in-progress; check the "last update" data at the top for new versions.

<p/>
<b>II. Basic Server Configuration</b><br/>
<ol>
  <li><b>Domain name</b>
  <ol type="a">
     <li>Add domain name for new server to our existing DNS records.
      <p/>
      Currently, these are maintained by Charles, and are spread
       across 3 different DNS servers around the country.&nbsp;
      (ns1.screenporch.com, ns2.screenporch.com, ns3.screenporch.com.)&nbsp;
      The advantage with this approach is that we can change DNS
      resolution for a server very quickly if we need to migrate to a failover server.&nbsp;
      (Commercial DNS services tend to not be able to react as quickly.)

      <p/>
      The disadvantage is, Charles is an individual, with no other
      support behind him.

      <p/>
      Another option would be to provide Scott(?) with direct access
      to the master DNS server, and appropriate training, if needed.
     </li>

     <li>Use the ServerBeach control panel to register the reverse-DNS
       lookup of the new server's IP address, to our domain name.
   </ol>


<p/>
   
  <li><b>Root login</b>.&nbsp;
   <ol type="a">
   <li>Edit /etc/ssh/sshd_config, make sure it is using "Protocol 2", and 
      "PermitRootLogin yes".&nbsp; Restart sshd with:
      <ul>
      <table>
      <tr><td><tt>ps -ef | grep "/usr/sbin/sshd"&nbsp;&nbsp;&nbsp;</tt></td>   <td>(to find the process id <i>nnnnn</i> of sshd)</td>
      <tr><td><tt>kill -1 <i>nnnnn</i></td>   <td>(to restart sshd)</td>
      </table>
      </ul>

    <p/>
    <li>Change the root password.

    <p/>
    <li>Edit /root/.bash_profile, add this line to the bottom:
<pre>
   export PS1='p7:$PWD # '
</pre>
   where p7 is the short name of the new box.&nbsp;
   Logout and log back in.

<p/>
   <li>Create /root/.forward, and make it contain the addresses
   that 'root' email should go to, e.g.
<pre>
   croth@thedance.net,scott.tudd@gmail.com
</pre>

<p/>
<li>Create ssh key:
<pre>
   mkdir .ssh
   chmod 700 .ssh
   ssh-keygen -b 1024 -t rsa
</ol>
(press Enter at all the prompts)

<p/>
<li><b>Install git</b>.&nbsp; As root,
<pre>
   yum install git
</pre>

<p/>
<li><b>Set up Iptables firewall</b>.&nbsp;
<ol type="a">
<li>As root, check out the COE scripts repository, 
and copy the firewall scripts to /usr/local/bin:
<pre>
   git clone git@github.com:wcroth55/coe.git COE
   cp COE/firewall          /usr/local/bin
   cp COE/firewallRules.txt /usr/local/bin
   chmod 700                /usr/local/bin/firewall*
</pre>
(If the checkout does not work, ask Charles to add the contents  of .ssh/id_rsa.pub to
the list of ssh keys for the COE project.&nbsp;
Or just check out the project to your laptop, and copy the firewall files over
manually.)

<li>Edit /usr/local/bin/firewallRules.txt to
assign roles and IP addresses as appropriate.&nbsp;
See the comments at the top of the file for details.&nbsp;
<p/>
Make sure to include port 22 access to ServerBeach 
staff, seems like we always have to ask them which
IP address ranges to include.

<p/>
<li>Edit /etc/rc.d/rc.local, and add the following line to the bottom:
<pre>
   /usr/local/bin/firewall
</pre>

This will ensure the firewall gets turned on when the server reboots.

<p/>
<b>Note:</b> make sure to do:
<pre>
   chmod 755 /etc/rc.d/rc/local
</pre>

<p/>
<li>
<b>Important: turn on the firewall now!</b>&nbsp;
As root, just type:
<pre>
   firewall
</pre>
</ol>

<p/>
<li><b>Update all installed software</b>.&nbsp;
As root, do:
<pre>
   yum update
</pre>
and answer "yes" or the equivalent to all questions.


<p/>
<li><b>Time</b>
<ol type="a">
<li>Set time zone to Eastern US:
<pre>
   rm -f /etc/localtime
   ln -s /usr/share/zoneinfo/America/New_York /etc/localtime
</pre>

<p/>
<li>Enable ntpd time correction:
<pre>
   service ntpd start
</pre>

Edit /etc/rc.d/rc.local, and add the line:
<pre>
   /sbin/service ntpd start
</pre>

</ol>

<p/>
<li><b>Web server</b>
  <ol type="a">
  <li>Install:
<pre>
   yum install httpd.x86_64
</pre>

  <p/>
  <li>Make configuration changes.
   <ol type="i">
   <li>Copy local.conf from the git COE checkout, to /etc/httpd/conf.
   <li>Edit local.conf to put in correct ServerAdmin, ServerName, NameVirtualHost IP address.
   <li>Edit /etc/httpd/conf/httpd.conf:<br/>
      Comment out "AddDefaultCharset UTF-8"<br/>
      Add "Include conf/local.conf" (to the very end)<br/>
      Comment out "UserDir disabled"<br/>
   </ol>

   <li><tt>mkdir /etc/httpd/conf/vhosts</tt>
 
   <li>Edit /etc/rc.d/rc.local, add
<pre>
   /sbin/service httpd start
</pre>

   <li>Start web server:
<pre>
   service httpd start
</pre>

  </ol>

  <p/>
  <li>Add additional disk(s)</b>, if relevant.&nbsp;
      Example shown for adding a second disk.
<pre>
   fdisk /dev/sdb
   n
   p
   1
         <i>(press Enter to accept default)</i>
         <i>(press Enter to accept default)</i>
   w

   mkfs -t ext3 /dev/sdb1
   mkdir /disk2
   mount -t ext3 /dev/sdb1 /disk2
   df -k
</pre>
(The last line should show both disks, sda1 and sdb1, as mounted and available.)

<p/>
And (of course), add the mount command to /etc/rc.d/rc.local:
<pre>
   /bin/mount -t ext3 /dev/sdb1 /disk2
</pre>

   

</ol>

<p/>
<b>III. Install software needed to build and run Caucus</b>.

<ol>
<li><b>MySQL</b>.&nbsp;

<pre>
   yum install mysql
   yum install mysql-server.x86_64
   yum install mysql-devel.x86_64
   yum install telnet    <i>(this is just the client, not the telnet server)</i>
</pre>

Edit /etc/my.cnf, and (before the [mysqld_safe] section),
add these lines to make MySQL use a larger share of RAM:
<pre>
   #---Optimization changes
   innodb_log_file_size = 100M
   innodb_buffer_pool_size = 4000M
   innodb_flush_method = O_DIRECT
   innodb_flush_log_at_trx_commit = 2
   table_cache = 8192
   open_files_limit = 8192
   innodb_support_xa = off
   max_connections=500
</pre>

Then continue with starting and initializing MySQL:

<pre>
   service mysqld start
   mysql_secure_installation
</pre>
and answer Y to all of the questions.
<p/>
Verify that you can use MySQL:
<pre>
   mysql -u root -p
   show databases;
   quit;
</pre>

Start MySQL at boot: edit /etc/rc.d/rc.local, and add
<pre>
   /sbin/service mysqld start
</pre>

<p/>
<li><b>C development tools</b>
<pre>
   yum install subversion.x86_64
   yum install gcc.x86_64
</pre>

<p/>
<li><b>Java</b>
<pre>
   yum install java-1.7.0-openjdk.x86_64
</pre>

<p/>
<li><b>ImageMagick</b>
<pre>
   yum install ImageMagick.x86_64
</pre>

</ol>

<b>IV. Build and Test Caucus</b>
<ol>
<li>Make a userid to test in, e.g.
<pre>
   useradd -u 1000 -d /home/croth croth
   passwd croth
</pre>

<li>Export Caucus source.&nbsp;
Logged in as a normal (non-root) user, do:
<pre>
   svn export svn://caucus.com/caucus/trunk C5
</pre>

<li>Build.&nbsp; For a CentOS 6.6 64-bit system, use:
<pre>
   cd C5
   export CPPFLAGS="-D_GNU_SOURCE"
   ./configure --mysqllib=/usr/lib64/mysql/libmysqlclient.so --nosasl2
   make
   mv caucus5.tar ~
   cd
   chmod 711 .
</pre>

<li>As root, create a test userid, e.g.
<pre>
   useradd -u 1001 -d /home/p7test p7test
   passwd p7test
</pre>

<li>Login to the test userid, and install Caucus, e.g.:
<pre>
   su - p7test
   tar xvf /home/croth/caucus5.tar
   ./install
</pre>

Suggested hostname: p7test.coexploration.org<br/>
Suggested email participation id: p7test (but just give junk when asked for its password,
not testing that just yet)

<pre>
   cd SWEB
   ./swebd ./swebd.conf
</pre>

<p/>
<li>Configure test id for http.
<ol type="a">
<li>Add p7test.coexploration.org to DNS resolution, to point at p7.coexploration.org.
<li>As root, create /etc/httpd/conf/vhosts/p7test.coexploration.org, containing:
<pre>
&lt;VirtualHost 66.135.33.165&gt;
   ServerName p7test.coexploration.org
   DocumentRoot /home/p7test/public_html

   &lt;Directory /home/p7test/SWEB&gt;
      Options All
      AllowOverride All
      allow from all
   &lt;/Directory&gt;
   
   &lt;Directory /home/p7test/REG&gt;
      Options All
      AllowOverride All
      allow from all
   &lt;/Directory&gt;
   
   &lt;Directory /home/p7test/public_html&gt;
      Options -Indexes
   &lt;/Directory&gt;
   
   ScriptAlias  /sweb/   /home/p7test/SWEB/
   ScriptAlias  /reg/    /home/p7test/REG/
   
&lt;/VirtualHost&gt;
</pre>

<li>As root, restart http:
<pre>
   service httpd restart
</pre>

</ol>
<li>Point a browser at http://p7test.coexploration.org.&nbsp;<br/>
Create the manager user (specified in the install dialog),
login, create a conference, and create an item.

<p/>
<li>Additional configuration.&nbsp; Edit the Caucus configuration
file (e.g. /home/p7test/SWEB/swebd.conf) and change values
needed for your specific site.&nbsp;
For example:
<pre>
   Config convertDir /usr/bin

   Config url  /usr/bin/curl
</pre>

</ol>

</ol>

<p/>
<b>V. Antivirus</b>
The primary purpose of the antivirus software is to detect
virus-laden software that is uploaded by users <b>into</b>
Caucus.

<ol>
<li>Prepare clamav user and group.
<pre>
   groupadd clamav
   useradd -g clamav -s /bin/false -c "Clam AntiVirus" clamav
</pre>

<li>Download source for the free ClamAV from 
http://www.clamav.net/download.html#sourcecode

<li>Build.&nbsp;
As root, do:
<pre>
   tar xvf clamav-0.98.5.tar.gz
   cd clamav-0.98.5
   ./configure
   make
   make install

   mkdir -p /usr/local/share/clamav
   chown clamav:clamav /usr/local/share/clamav

   touch /var/log/clamd.log
   chown clamav /var/log/clamd.log

   touch /var/log/freshclam.log
   chown clamav /var/log/freshclam.log
</pre>

<li>Configure:
<ol type="a">
<li>Copy /usr/local/etc/clamd.conf.sample to /usr/local/etc/clamd.conf.&nbsp;
Edit the latter file.<br/>
Uncomment the "TCPSocket" line.</br>
Comment out "Example".<br/>
Change the commented-out LogFile line to:
<pre>
   LogFile /var/log/clamd.log
</pre>

<p/>
<li>Copy /usr/local/etc/freshclam.conf to /usr/local/etc/freshclam.conf.<br/>
Comment out the "Example" line.<br/>
Uncomment the UpdateLogFile line.<br/>
Change the DatabaseMirror line to (uncommented):
<pre>
   DatabaseMirror db.us.clamav.net
</pre>

<p/>
<li>Run 'freshclam' virus signature update tool:
<pre/>
   freshclam
</pre>

<p/>
<li>Run 'freshclam' once a day.&nbsp; Add (something like) the following
to your crontab:
<pre>
   10 3 * * * /usr/local/bin/freshclam --quiet
</pre>

<p/>
<li>Start the 'clamd' daemon:
<pre>
   clamd
</pre>

<p/>
<li>Run 'clamd' at boot time.&nbsp;
Edit /etc/rc.d/rc.local and add:
<pre>
   /usr/local/sbin/clamd
</pre>

<p/>
<li>Connect to Caucus.<br/>
Copy the 'viruschecker' file from the COE git repository, to /usr/local/bin.
<pre>
   chmod 755 /usr/local/bin/viruschecker
</pre>

</ol>
</ol>

<p/>
<b>VI. Configure Email</b>
<ol>
<li>Create a new userid 'mailhandler', e.g. something like:
<pre>
   adduser -u 1003 -d /home/mailhandler mailhandler
</pre>

<li>Set it to throw away email it receives.&nbsp;
(Later we'll reconfigure it to handle incoming
email-to-a-Caucus-item.)
<pre>
   su - mailhandler
   echo "/dev/null" >.forward
   chmod 755 .forward
</pre>

<p/>
<li>Configure postfix email.&nbsp;
Add the text below to the end of /etc/postfix/main.cf,
substituting in the appropriate values for the underlined text:

<pre>
   inet_interfaces = all
   myhostname=<u>p7.coexploration.org</u>
   mydestination = $myhostname, localhost.$mydomain, localhost
   mynetworks = <u>66.135.33.165</u>, 127.0.0.1
   myorigin = $myhostname
   #alias_maps = hash:/etc/postfix/aliases
   #alias_database = hash:/etc/postfix/aliases
   smtpd_sasl_auth_enable = yes
   broken_sasl_auth_clients = yes
   smtpd_recipient_restrictions = permit_mynetworks, permit_sasl_authenticated, reject_unauth_destination
   max_use=10
   local_recipient_maps =
   luser_relay = mailhandler
   #canonical_maps = hash:/etc/postfix/sender_canonical
</pre>

<b>Note:</b> configuration lines for main.cf <b>must not</b> indented.&nbsp;
That has a special meaning to postfix (it's a kind of continuation line).

<p/>
Then restart Postfix:
<pre>
   service postfix restart
</pre>

<p/>
<li>Allow incoming email.&nbsp;
Edit /usr/local/bin/firewallRules.txt, and change the ANYONE rule at the bottom
to include port 25, e.g.
<pre>
   ANYONE gets 80 443 25
</pre>
Then restart the firewall.&nbsp; As root, run:
<pre>
   firewall
</pre>

<p/>
<li><b>Test it!</b>&nbsp;
Make sure you can send email from root.


</ol>



<p/>
<b>VII. MySQL Replication</b><br/>
This section assumes that we have two servers in play: server A, which will
be the master, and server B, which is (mostly) used as a failover server
in case A should die or become unavailable.

<p/>
The standard MySQL server (service) on B will be used as the MySQL slave
in the replication scheme.&nbsp;
Other databases (i.e. other than those on A) may exist on B, but be
very careful not to use the names of any databases that could potentially
be created on A.

<ol>
<li><b>Clean up the master</b>.&nbsp;
On A: remove the test Caucus space (and database!).&nbsp;
This makes setting up the MySQL replication much easier.&nbsp;
<p/>
E.g. for a test Caucus space <b>p7test</b>, do:
<pre>
   su - p7test
   ps x | grep "swebd "
   kill -1 <i>(fill in the process id of the swebs task)</i>
   exit

   userdel -r p7test

   mysql -u root -p
   drop database caucus_p7test;
   quit;
</pre>

<p/>
<li><b>Prepare master for replication</b>.&nbsp;
Shutdown the MySQL service:
<pre>
   service mysqld stop
</pre>

Create the binary logging directory needed for replication:
<pre>
   mkdir /var/lib/mysql/log-bin
   chown mysql:mysql /var/lib/mysql/log-bin
</pre>

Edit /etc/my.cnf, and somewhere in the "[mysqld]" section, add these lines:
<pre>
   server-id=1
   log-bin=mysql-bin
   binlog-ignore-db=mysql
   binlog-ignore-db=information_schema
</pre>

(If there are any other databases on A that should <b>not</b> be replicated
to B, add them as well with additional binlog-ignore-db lines.&nbsp;
For a new server, these two should be the only such databases.)

<p/>
Restart MySQL, and verify that it is actually working:
<pre>
   service mysqld start
   mysql -u root -p
   select now();
   quit;
</pre>
If there is a problem, look in /var/lib/mysql  or /var/log for the MySQL logs.

<p/>
Create the 'replicator' userid in MySQL:
<pre>
   mysql -u root -p
   grant replication slave on *.* to replicator@'slaveserver.com' identified by 'slavepassword';
   flush privileges;
   show master status;
   quit;
</pre>
<b>Save the output</b> from 'show master status'; you'll need it later.

<p/>
<li><b>Open firewall on master to access from slave</b>.&nbsp;
<p/>
On A, edit /usr/local/bin/firewallRules.txt, and add rules to allow
incoming ssh and MySQL requests from B.&nbsp;  E.g. for a slave B
named p8, add lines something like:
<pre>
   P8 is 64.34.196.252

   P8     gets 22 3306
</pre>
<p/>
As root, run 'firewall' to update the firewall according
to the new rules.

<p/>
<li><b>Clean up the slave.</b>&nbsp;
On B, remove the test Caucus site, if any.&nbsp;
(Follow the instructions from VII.1.)

<p/>
<li><b>Prepare the slave.</b>&nbsp;
on B, edit /etc/my.cnf, and in the [mysqld] section, add:
<pre>
   server-id=2
   master-host = masterserver.com
   master-user = replicator
   master-password = slavepassword    <i>(from above!)</i>
   master-connect-retry = 60
</pre>

Then restart the slave:
<pre>
   service mysqld restart
</pre>

<p/>
<li><b>Connect the slave to the master.</b>&nbsp;
On B:

<pre>
   mysql -u root -p
   slave stop;
   reset slave;
   change master to master_host='masterserver.com',
   master_user='replicator',
   master_password='slavepassword',
   master_log_file='(as shown in master status)',
   master_log_pos=(as shown in master status);

   start slave;
   show slave status\G;
   quit;
</pre>

You should see "Slave_IO_Running: Yes" and "Slave_SQL_Running: Yes".&nbsp;
Anything else means something is wrong 
(e.g. the firewall is blocking the slave, the wrong ports are in use, etc. etc.).&nbsp;
Check the mysql logs for details.

<p/>
<li><b>Create replication test</b> and verify that replication is working.
<p/>
On the master A, create a simple database that will be used to continuously
verify that replication is working:
<pre>
   create database replication_test;
   grant all on replication_test.* to replication_test identified by 'tester';
   use replication_test;
   CREATE TABLE last_update (
     now datetime DEFAULT NULL
   ) ENGINE=InnoDB DEFAULT CHARSET=latin1;
   insert into last_update (now) values (now());
   quit;
</pre>

Look on the slave; you should see this database suddenly appear.

<p/>
<li><b>Automate replication testing.</b>&nbsp;
On the master A, create a script /usr/local/bin/replication_tester,
containing:
<pre>
   mysql -u replication_test --password='tester' replication_test \
         -e "update last_update set now=now();"
</pre>

Make it executable ("chmod 755 /usr/local/bin/replication_tester")
and run it from cron every 10 minutes.

<p/>
Copy the file replication_verifier (from the git repository) to
<b>slave B's</b> /usr/local/bin, and make it executable.&nbsp;
Edit the script to change or add the email addresses that should
receive notices about replication failures.

<p/>
On B, use cron to run the replication_verifier script every 10 minutes, a few minutes
after the the master runs replication_tester.&nbsp;
<p/>
You should receive an email once a day to verify positively that replication
is working.&nbsp;
You will also receive an email <b>any time</b> replication_verifier fails.&nbsp;
A single failure is not important; if there's a lot of activity on the master
<b>or</b> slave, replication can slow down a bit.&nbsp;
But multiple failures in a row are worth investigating.

</ol>

<p/>
<b>VIII. How to move an existing Caucus site</b><br/>
In order to move an existing site, you will have to shut it down
for a period of time, depending on the size of the site.&nbsp;
Generally it's best to allow a couple of hours.

<p/>
The instructions below assume that the Caucus userid is 'caucus'
and the database is called 'caucus_caucus'.&nbsp;
Modify accordingly.
<ol>
<li><b>Stop old site processes</b>.&nbsp;  E.g.  on the old site, for site 'caucus', do:
<pre>
   su - caucus
   crontab -r

   ps x
   echo "========"

   ps x | grep -e swebs -e "swebd " -e java | grep -v grep | \
   (while read a b; do
       kill -1 $a
    done)

   ps x
</pre>
You should see a "before and after" list of Caucus processes, 
split around the "=====" line.&nbsp;
(There shouldn't be any Caucus-related processes running after
the split.&nbsp;
If there are, try it again, or kill remaining processes
manually.)

<p/>
<li><b>Change DNS.</b>&nbsp;
Change the DNS entry for the site to point to the new server.

<p/>
<li><b>Dump the MySQL database.</b>
Starting in the home directory of the 'caucus' userid:
<pre>
   mysqldump -u caucus -p caucus_caucus >caucus_caucus.sql
</pre>

<p/>
<li><b>Archive the entire site.</b>
Starting in the home directory of the 'caucus' userid:
<pre>
   rm -rf SOCKET
   tar cvf /tmp/caucus.tar *
   gzip    /tmp/caucus.tar
</pre>

<p/>
<li><b>Transfer to the new server.</b>&nbsp;
Typically, this means scp the /tmp/caucus.tar.gz file to somewhere on the new server.&nbsp;
If you are "pushing" it from the old server to the new server, you will also need
to open the firewall on the new server (to allow access from the old server) for port 22
first.

<p/>
<li><b>Create empty site on the new server.</b>&nbsp;
Go through the usual procedure of creating an empty Caucus site,
on the new server.&nbsp;
Use the <b>same</b> name as the old site on the old server.&nbsp;
Make sure it works.
<p/>
Then make sure to <b>shut it down</b> (see VIII.1 again.)

<p/>
<li><b>Overlay the new (empty) site</b> with the old data.&nbsp;
Assuming that the old data has been placed in /tmp/oldcaucus.tar.gz, 
as the new 'caucus' user on the new server do:
<pre>
   tar xvfz /tmp/oldcaucus.tar.gz
   mysql -u caucus -p caucus_caucus -e "source caucus_caucus.sql;"
</pre>

<p/>
<li><b>'Update' the site from the new kit.</b>&nbsp;
IOW, "re-install" Caucus over top of the old data.&nbsp;  E.g.
<pre>
   tar xvf whereverTheNewKitIs.tar  <i>(see IV.5)</i>
   ./install                        <i>(answer all the re-install questions)</i>

   cd SWEB
   ./swebd ./swebd.conf             <i>(Restart the site)</i>
</pre>

<p/>
<li><b>Clean up.</b>&nbsp; On the new server:
<pre>
   rm -f /home/caucus/caucus_caucus.sql
   rm -f /tmp/oldcaucus.tar.gz
</pre>

<p/>
Check the output of 'crontab -l' for the moved site, 
and make sure it does what you expect it to do.

<p/>
<li><b>Start site at boot.</b>&nbsp;
Follow the usual directions to start the Caucus site at boot-time, e.g.
edit /etc/rc.d/rc.local and add:
<pre>
   rm -f /home/caucus/SOCKET/sweb /home/caucus/SOCKET/sweb0*
   /home/caucus/SWEB/swebd /home/caucus/SWEB/swebd.conf
</pre>

(Also <b>remove</b> the equivalent lines from the old server!)
</ol>

<p/>
<b>IX. Rsync file backup</b><br/>
In addition to the automatic backup of the MySQL data via replication,
the slave (aka "failover") server should also have regular backups
of all other relevant files, notably including:
<ul>
<li>Uploaded/attached files in Caucus.
<li>Separate 'static' web sites.
<li>Any other files that you would need to properly "fail over" to the slave
   in the event that the master died.
</ul>

<p/>
The wonderful thing about rsync is that it is smart enough to copy only
the new files, the <b>changed</b> files (and <b>changed parts</b> of those files) over.&nbsp;
So it uses very near the theoretical minimum bandwidth to do the 
synchronization.
<p/>
The steps are:

<ol>
<li><b>Open firewall</b>, on the slave, <b>to</b> the master.&nbsp;
Look up the master's IP address, and add it to the slave's /usr/local/bin/firewallRules.txt.
<p/>
E.g. for master p7 and slave p8, on p8 do:
<pre>
   nslookup
   p7.coexploration.org
   ^C                       <i>(control-C)</i>
</pre>
Edit the firewallRules.txt, add 
<pre>
   P7 is <i>(IP address from previous step)</i>
   P7 gets 22
</pre>

Then run 'firewall'.

<p/>
<li><b>Provide for password-less ssh login from master to slave</b>.&nbsp;
<p/>
On the master, look for /root/.ssh/id_rsa.pub.&nbsp;
If it does not exist, do:
<pre>
   ssh-keygen -b 1024 -t rsa
</pre>
and just press Enter at all the prompts.

<p/>
Then append the contents of that file on the master,
<b>to</b> the end of the file /root/.ssh/authorized_keys2 on the slave.&nbsp;
If the .ssh directory doesn't exist, do:
<pre>
   mkdir     /root/.ssh
   chmod 700 /root/.ssh
</pre>

If the file authorized_keys2 doesn't exist, just copy the id_rsa.pub file to authorized_keys2.

<p/>
Check that it works.&nbsp;
From the master, you should be able to do
<pre>
   ssh root@slavehostname.org
</pre>
and get a login prompt, <b>without</b> entering a password.

<p/>
<li><b>Install the backup script.</b>&nbsp;
Copy the file backupToSlave.sh from the COE repository to /root on the master.&nbsp;
Make sure it is executable, e.g.
<pre>
   chmod 700 /root/backupToSlave.sh
</pre>

Edit the file so that the symbols 'master' and 'idhost' are correct.
<p/>
Set the file to run (at least) once a day, via crontab on the master.&nbsp;
I like to do this via a file, e.g:
<pre>
   crontab -l >crontab_file
   <i>(edit crontab_file until it's right)</i>
   crontab crontab_file
</pre>

The crontab entry for the backup script would look something like:
<pre>
   10 3 * * * /root/backupToSlave.sh
</pre>
(which runs at 10 minutes after 3 am, every day)

<p/>
<li><b>Prepare the slave.</b>&nbsp;  On the slave, do:
<pre>
   mkdir /root/p7     <i>(or whatever value you have for 'master' in the backup script)</i>
   mkdir /root/p7/usr
   mkdir /root/p7/usr/local
   mkdir /root/p7/usr/local/bin
</pre>

<p/>
<li><b>Test the backups.</b>&nbsp;
Make sure that the backup script is run from cron.&nbsp;
Then later that day, on the slave, examine the contents of (say) /root/p7 to see if
it has the contents you expect.&nbsp;
<p/>
Also look on the master, in /tmp, 
for a file named "backup" with the date and time of the expected backup as part of the name.&nbsp;
It should contain a list of the files that were transferred, as well as the starting and ending
time of the backup process.

<p/>
<li><b>Using the backups.</b>&nbsp;
If you ever need the backups, most of the work can be achieved by simply moving the
appropriate contents over, e.g.
<pre>
   mv /root/p7/home/* /home
   cp /root/p7/usr/local/bin/* /usr/local/bin
</pre>

You may need to create userids on the slave to match those that were on
the master: examine /root/p7/etc/passwd to see the list of ids that
were on the master.

<p/>
Similarly, you may need the httpd configuration files from /root/p7/etc/httpd/conf
and /root/p7/etc/httpd/conf/vhosts copied, and lightly edited (e.g. to put in the
correct IP address) into the equivalent locations on the slave.
 



</ol>



<p/>
<b>X. Next?</b>
<ol>
<li>Docker?
<li>...?
</ol>


</body>
</html>
